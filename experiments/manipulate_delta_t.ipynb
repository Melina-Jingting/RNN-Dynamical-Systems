{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=32.91423034667969\n",
      "step=1000, loss=0.022265421226620674\n",
      "step=2000, loss=0.010654343292117119\n",
      "step=3000, loss=0.003794194897636771\n",
      "step=4000, loss=0.0053800721652805805\n",
      "step=5000, loss=0.007801481056958437\n",
      "step=6000, loss=0.0034492823760956526\n",
      "step=7000, loss=0.0013229420874267817\n",
      "step=8000, loss=0.0007672627689316869\n",
      "step=9000, loss=0.0036113986279815435\n",
      "step=0, loss=20.017702102661133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     55\u001b[0m     experiment_dts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m]\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_dts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_existing_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(experiment_dts, n_neurons, n_training_steps, log_every, use_existing_models)\u001b[0m\n\u001b[1;32m     43\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_hyperparameters)\n\u001b[1;32m     44\u001b[0m     opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39minit(model)\n\u001b[0;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_training_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Final evaluation and plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Computational Neuroscience/RNN-Dynamical-Systems/src/training.py:38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, iter_data, optim, opt_state, model_id, dt, n_training_steps, log_every)\u001b[0m\n\u001b[1;32m     35\u001b[0m   step \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_training_steps), iter_data):\n\u001b[0;32m---> 38\u001b[0m     loss, model, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Computational Neuroscience/RNN-Dynamical-Systems/.venv/lib/python3.12/site-packages/equinox/_jit.py:248\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m         marker, _, _ \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached(\n\u001b[1;32m    245\u001b[0m             dynamic_donate, dynamic_nodonate, static\n\u001b[1;32m    246\u001b[0m         )\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jitting:\n\u001b[0;32m--> 248\u001b[0m         \u001b[43mmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_until_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Catch Equinox's runtime errors, and re-raise them with actually useful\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# information. (By default XlaRuntimeError produces a lot of terrifying\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# but useless information.)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m         last_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m last_stack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# callback necessarily executed in the same interpreter as we are in\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# here?\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, optax, sys\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Adjust based on your current working directory\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from src.config import get_run_config, get_data_hyperparameters, get_model_hyperparameters, get_optimizer_hyperparameters\n",
    "from src.data_utils import nbatch_nbit_memory_dataloader, build_nbatch_nbit_memory\n",
    "from src.model_utils import compile_metadata, generate_model_id, save_model_metadata, load_model, save_model, clear_model_cache\n",
    "from src.training import train_model\n",
    "from src.plotting import plot_dts_experiment\n",
    "from src.models import ContinuousTimeRNN\n",
    "\n",
    "def run_experiment(experiment_dts, n_neurons=2, n_training_steps=10000, log_every=1000, use_existing_models=False):\n",
    "    \"\"\"Run the n-bit memory task experiment.\"\"\"\n",
    "    config = get_run_config()\n",
    "    models = []\n",
    "\n",
    "    for dt in experiment_dts:\n",
    "        # Hyperparameters\n",
    "        data_hyperparameters = get_data_hyperparameters(config, dt=dt)\n",
    "        model_hyperparameters = get_model_hyperparameters(config)\n",
    "        optimizer_hyperparameters = get_optimizer_hyperparameters()\n",
    "\n",
    "        # Compile metadata and generate model ID\n",
    "        metadata = compile_metadata(config, model_hyperparameters, data_hyperparameters, optimizer_hyperparameters)\n",
    "        model_id = generate_model_id(metadata)\n",
    "        save_model_metadata(metadata, model_id)\n",
    "\n",
    "        # Check if the model already exists\n",
    "        model_exists = os.path.exists(f\"saved_models/{model_id}/model.eqx\")\n",
    "        if use_existing_models and model_exists:\n",
    "            model = load_model(model_id, ContinuousTimeRNN)\n",
    "        else:\n",
    "            clear_model_cache(model_id)\n",
    "            model = ContinuousTimeRNN(**model_hyperparameters)\n",
    "\n",
    "        # Data loader\n",
    "        iter_data = nbatch_nbit_memory_dataloader(**data_hyperparameters)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = optax.adam(**optimizer_hyperparameters)\n",
    "        opt_state = optimizer.init(model)\n",
    "        model = train_model(model, iter_data, optimizer, opt_state, model_id=model_id, dt=dt, n_training_steps=n_training_steps, log_every=log_every)\n",
    "        models.append(model)\n",
    "\n",
    "    # Final evaluation and plotting\n",
    "    config[\"seed\"] = 0  # Reset seed for evaluation data\n",
    "    data_hyperparameters = get_data_hyperparameters(config, dt=1, batch_size=1)\n",
    "    input_coarse, target_coarse = build_nbatch_nbit_memory(**data_hyperparameters)\n",
    "    plot_dts_experiment(models, experiment_dts, input_coarse[0], target_coarse[0], T=25, n_neurons=n_neurons, plot_input=True, plot_dynamics=True, plot_output=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_dts = [1, 0.1, 0.01]\n",
    "    run_experiment(experiment_dts, n_neurons=2, n_training_steps=10000, log_every=1000, use_existing_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
